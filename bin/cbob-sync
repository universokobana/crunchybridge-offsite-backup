#!/bin/bash

# CBOB Sync - Sync backups from Crunchy Bridge to local or S3-compatible storage
# This is a rewrite of the original cbob_sync with improved error handling and features
# Supports: Local disk, Digital Ocean Spaces, Hetzner Object Storage, MinIO, etc.

set -euo pipefail

# Source common functions
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
if [ -f "${SCRIPT_DIR}/../lib/cbob_common.sh" ]; then
    source "${SCRIPT_DIR}/../lib/cbob_common.sh"
else
    source "/usr/local/lib/cbob/cbob_common.sh"
fi
if [ -f "${SCRIPT_DIR}/../lib/cbob_metrics.sh" ]; then
    source "${SCRIPT_DIR}/../lib/cbob_metrics.sh"
else
    source "/usr/local/lib/cbob/cbob_metrics.sh"
fi

# Help text
show_help() {
    cat << EOF
Usage: cbob sync [options]

Sync backups from Crunchy Bridge to local storage or S3-compatible object storage.

Options:
    --cluster ID        Sync only specific cluster (can be used multiple times)
    --parallel N        Number of parallel sync operations (default: 1)
    --bandwidth LIMIT   Bandwidth limit for AWS S3 sync (e.g., 10MB/s)
    --retry-count N     Number of retries for failed operations (default: 3)
    --retry-delay N     Initial retry delay in seconds (default: 5)
    --skip-validation   Skip post-sync validation
    --estimate-only     Show estimated backup sizes without syncing
    --status            Show sync status for all clusters and exit
    --help, -h          Show this help message

Cluster Selection:
    By default, syncs ALL primary clusters (replicas are auto-ignored).
    Use CBOB_IGNORE_CLUSTERS to exclude specific clusters from sync.

    CBOB_IGNORE_CLUSTERS    - Comma-separated cluster IDs to ignore
    CBOB_CRUNCHY_CLUSTERS   - (Legacy) Explicit list of clusters to sync

    Priority order:
        1. --cluster flags (if specified)
        2. CBOB_CRUNCHY_CLUSTERS (legacy explicit list)
        3. Auto-discover all primary clusters minus CBOB_IGNORE_CLUSTERS

Destination Configuration:
    Set CBOB_DEST_TYPE=s3 to sync to S3-compatible storage instead of local disk.
    Required variables for S3 destination:
        CBOB_DEST_ENDPOINT    - S3 endpoint URL (e.g., https://fra1.digitaloceanspaces.com)
        CBOB_DEST_BUCKET      - Destination bucket name
        CBOB_DEST_ACCESS_KEY  - Access key for destination
        CBOB_DEST_SECRET_KEY  - Secret key for destination
    Optional:
        CBOB_DEST_REGION      - Region (default: us-east-1)
        CBOB_DEST_PREFIX      - Path prefix in bucket (default: none)

Examples:
    cbob sync                           # Sync all primary clusters (auto-discover)
    cbob sync --cluster abc123          # Sync only cluster abc123
    cbob sync --status                  # Show sync status for all clusters
    cbob sync --parallel 4              # Sync up to 4 clusters in parallel

    # Ignore specific clusters:
    CBOB_IGNORE_CLUSTERS=sandbox123,staging456 cbob sync

    # Sync to Digital Ocean Spaces:
    CBOB_DEST_TYPE=s3 \\
    CBOB_DEST_ENDPOINT=https://fra1.digitaloceanspaces.com \\
    CBOB_DEST_BUCKET=my-backups \\
    cbob sync

EOF
}

# Parse command-line options
parse_options() {
    SYNC_CLUSTERS=()
    SYNC_PARALLEL=1
    SYNC_BANDWIDTH=""
    SYNC_RETRY_COUNT=3
    SYNC_RETRY_DELAY=5
    SYNC_SKIP_VALIDATION=false
    SYNC_ESTIMATE_ONLY=false
    SYNC_STATUS_ONLY=false
    
    while [[ $# -gt 0 ]]; do
        case "$1" in
            --cluster)
                SYNC_CLUSTERS+=("$2")
                shift 2
                ;;
            --parallel)
                SYNC_PARALLEL="$2"
                shift 2
                ;;
            --bandwidth)
                SYNC_BANDWIDTH="$2"
                shift 2
                ;;
            --retry-count)
                SYNC_RETRY_COUNT="$2"
                shift 2
                ;;
            --retry-delay)
                SYNC_RETRY_DELAY="$2"
                shift 2
                ;;
            --skip-validation)
                SYNC_SKIP_VALIDATION=true
                shift
                ;;
            --estimate-only)
                SYNC_ESTIMATE_ONLY=true
                shift
                ;;
            --status)
                SYNC_STATUS_ONLY=true
                shift
                ;;
            --help|-h)
                show_help
                exit 0
                ;;
            *)
                error "Unknown option: $1"
                ;;
        esac
    done
}

# Validate environment
validate_environment() {
    # Check if running as postgres user
    if [ "$(whoami)" != 'postgres' ]; then
        error "$0 must be run as postgres user. Run: sudo -u postgres $0"
    fi

    # Check dependencies
    check_dependencies "aws" "jq" "curl" "flock"

    # Load and validate configuration
    load_config

    # Validate base configuration (CBOB_CRUNCHY_CLUSTERS is now optional)
    validate_config "CBOB_CRUNCHY_API_KEY"

    # Validate destination-specific configuration
    if is_dest_s3; then
        info "Destination: S3-compatible storage (${CBOB_DEST_ENDPOINT:-not set})"
        validate_dest_s3_config
    else
        info "Destination: Local storage (${CBOB_TARGET_PATH:-not set})"
        validate_config "CBOB_TARGET_PATH"
    fi
}

# Get all clusters from Crunchy Bridge API
get_all_clusters() {
    local api_key="$1"

    debug "Fetching all clusters from Crunchy Bridge API..."

    local response=$(curl -s -f -X GET \
        "https://api.crunchybridge.com/clusters" \
        -H "Authorization: Bearer $api_key" \
        -H "Accept: application/json") || {
        error "Failed to fetch clusters from Crunchy Bridge API"
    }

    echo "$response"
}

# Get all primary (non-replica) clusters, excluding ignored ones
# Returns space-separated list of cluster IDs
get_primary_clusters() {
    local api_key="$1"
    local ignore_list="${2:-}"  # Comma-separated list of cluster IDs to ignore

    info "Fetching clusters from Crunchy Bridge API..."

    local all_clusters=$(get_all_clusters "$api_key")

    # Extract primary clusters (parent_id is null)
    local primary_ids=$(echo "$all_clusters" | jq -r '.clusters[] | select(.parent_id == null) | .id')

    # Build ignore array
    local -a ignore_array=()
    if [ -n "$ignore_list" ]; then
        IFS=',' read -ra ignore_array <<< "$ignore_list"
    fi

    # Filter out ignored clusters and build result
    local result=()
    local ignored_count=0
    local replica_count=0

    for cluster_id in $primary_ids; do
        local should_ignore=false

        # Check if in ignore list
        for ignored in "${ignore_array[@]}"; do
            if [ "$cluster_id" = "$ignored" ]; then
                should_ignore=true
                ((ignored_count++))
                debug "Ignoring cluster (in ignore list): $cluster_id"
                break
            fi
        done

        if [ "$should_ignore" = false ]; then
            result+=("$cluster_id")
        fi
    done

    # Count replicas for info
    replica_count=$(echo "$all_clusters" | jq '[.clusters[] | select(.parent_id != null)] | length')

    info "Found $(echo "$all_clusters" | jq '.clusters | length') total clusters"
    info "  - Primary clusters: ${#result[@]}"
    info "  - Replicas (auto-ignored): $replica_count"
    info "  - Manually ignored: $ignored_count"

    echo "${result[*]}"
}

# Get cluster information from Crunchy Bridge API
get_cluster_info() {
    local cluster_id="$1"
    local api_key="$2"

    debug "Fetching cluster information for: $cluster_id"

    local response=$(curl -s -f -X GET \
        "https://api.crunchybridge.com/clusters/$cluster_id" \
        -H "Authorization: Bearer $api_key" \
        -H "Accept: application/json") || {
        error "Failed to fetch cluster information for $cluster_id"
    }

    echo "$response"
}

# Get backup credentials from Crunchy Bridge API
get_backup_credentials() {
    local cluster_id="$1"
    local api_key="$2"
    
    debug "Fetching backup credentials for: $cluster_id"
    
    local response=$(curl -s -f -X POST \
        "https://api.crunchybridge.com/clusters/$cluster_id/backup-tokens" \
        -H "Authorization: Bearer $api_key" \
        -H "Accept: application/json") || {
        error "Failed to fetch backup credentials for $cluster_id"
    }
    
    echo "$response"
}

# Estimate backup size
estimate_backup_size() {
    local bucket="$1"
    local path="$2"
    local stanza="$3"
    
    info "Estimating backup size for stanza: $stanza"
    
    # Get total size of archive and backup directories
    local archive_size=$(aws s3 ls --summarize --recursive "s3://${bucket}${path}/archive/${stanza}/" 2>/dev/null | grep "Total Size:" | awk '{print $3}') || echo "0"
    local backup_size=$(aws s3 ls --summarize --recursive "s3://${bucket}${path}/backup/${stanza}/" 2>/dev/null | grep "Total Size:" | awk '{print $3}') || echo "0"
    
    local total_size=$((archive_size + backup_size))
    
    info "  Archive size: $(human_readable_size $archive_size)"
    info "  Backup size: $(human_readable_size $backup_size)"
    info "  Total size: $(human_readable_size $total_size)"
    
    echo "$total_size"
}

# Tracking file for cluster sync status (evaluated at runtime after config is loaded)
get_sync_tracking_file() {
    echo "${CBOB_BASE_PATH:-/var/lib/cbob}/sync_tracking.json"
}

# Get last sync timestamp for a cluster (returns 0 if never synced)
get_last_sync_time() {
    local cluster_id="$1"
    local tracking_file=$(get_sync_tracking_file)
    if [ -f "$tracking_file" ]; then
        jq -r ".[\"$cluster_id\"].last_sync // 0" "$tracking_file" 2>/dev/null || echo "0"
    else
        echo "0"
    fi
}

# Update tracking after successful sync
update_sync_tracking() {
    local cluster_id="$1"
    local tracking_file=$(get_sync_tracking_file)
    local timestamp=$(date +%s)
    local datetime=$(date -Iseconds)

    # Create parent directory if it doesn't exist
    local tracking_dir=$(dirname "$tracking_file")
    if [ ! -d "$tracking_dir" ]; then
        mkdir -p "$tracking_dir"
    fi

    # Create tracking file if it doesn't exist
    if [ ! -f "$tracking_file" ]; then
        echo "{}" > "$tracking_file"
    fi

    # Update the tracking for this cluster
    local tmp_file=$(mktemp)
    jq --arg id "$cluster_id" \
       --argjson ts "$timestamp" \
       --arg dt "$datetime" \
       '.[$id] = {"last_sync": $ts, "last_sync_datetime": $dt}' \
       "$tracking_file" > "$tmp_file" && mv "$tmp_file" "$tracking_file"

    debug "Updated sync tracking for cluster $cluster_id"
}

# Sort clusters by last sync time (oldest first)
sort_clusters_by_priority() {
    local clusters=("$@")
    local sorted=()

    # Create array with cluster:timestamp pairs
    local pairs=()
    for cluster_id in "${clusters[@]}"; do
        local last_sync=$(get_last_sync_time "$cluster_id")
        pairs+=("$last_sync:$cluster_id")
    done

    # Sort by timestamp (oldest first) and extract cluster IDs
    while IFS=: read -r ts cluster_id; do
        sorted+=("$cluster_id")
    done < <(printf '%s\n' "${pairs[@]}" | sort -t: -k1 -n)

    echo "${sorted[@]}"
}

# Show sync tracking status
show_sync_status() {
    local tracking_file=$(get_sync_tracking_file)
    if [ -f "$tracking_file" ]; then
        info "Cluster sync status (oldest first):"
        jq -r 'to_entries | sort_by(.value.last_sync) | .[] | "  \(.key): \(.value.last_sync_datetime // "never")"' "$tracking_file" 2>/dev/null || true
    fi
}

# Sync a single cluster
sync_cluster() {
    local cluster_id="$1"
    local start_time=$(start_timing)
    local bytes_before=0
    local bytes_after=0

    info "Starting sync for cluster: $cluster_id"
    
    # Record initial storage metrics
    record_storage_metrics "$cluster_id"
    bytes_before=$(get_metrics "storage" | jq -r ".[\"$cluster_id\"].total_size_bytes // 0" | tr -d '"')
    
    # Get last backup information
    local last_backup_response=$(curl -s -f -X GET \
        "https://api.crunchybridge.com/clusters/$cluster_id/backups?limit=1&order=desc" \
        -H "Authorization: Bearer $CBOB_CRUNCHY_API_KEY") || {
        warning "Failed to fetch last backup for cluster $cluster_id"
        return 1
    }
    
    local last_backup_name=$(echo "$last_backup_response" | jq -r '.backups[0].name // empty')
    if [ -z "$last_backup_name" ]; then
        warning "No backups found for cluster $cluster_id"
        return 1
    fi
    
    info "Last backup: $last_backup_name"
    
    # Get backup credentials
    local credentials=$(get_backup_credentials "$cluster_id" "$CBOB_CRUNCHY_API_KEY")
    
    # Extract AWS credentials
    local aws_config=$(echo "$credentials" | jq -r '.aws // empty')
    if [ -z "$aws_config" ] || [ "$aws_config" = "null" ]; then
        error "Failed to get AWS credentials for cluster $cluster_id"
    fi
    
    export AWS_REGION=$(echo "$credentials" | jq -r '.aws.s3_region')
    export AWS_SESSION_TOKEN=$(echo "$credentials" | jq -r '.aws.s3_token')
    export AWS_ACCESS_KEY_ID=$(echo "$credentials" | jq -r '.aws.s3_key')
    export AWS_SECRET_ACCESS_KEY=$(echo "$credentials" | jq -r '.aws.s3_key_secret')
    
    local bucket=$(echo "$credentials" | jq -r '.aws.s3_bucket')
    local repo_path=$(echo "$credentials" | jq -r '.repo_path')
    local stanza=$(echo "$credentials" | jq -r '.stanza')
    
    # Validate extracted values
    if [ -z "$stanza" ] || [ "$stanza" = "null" ]; then
        error "Missing stanza for cluster $cluster_id"
    fi
    
    # Estimate size if requested
    if [ "$SYNC_ESTIMATE_ONLY" = true ]; then
        estimate_backup_size "$bucket" "$repo_path" "$stanza"
        return 0
    fi
    
    # Build AWS sync command options
    local aws_sync_opts=()
    if [ -n "$SYNC_BANDWIDTH" ]; then
        aws_sync_opts+=("--cli-read-timeout" "60" "--cli-connect-timeout" "60")
    fi

    if [ "$CBOB_DRY_RUN" = "true" ]; then
        aws_sync_opts+=("--dryrun")
        info "Running in DRY-RUN mode"
    fi

    # Sync operations with retry logic
    info "Syncing archive for stanza: $stanza"
    retry_with_backoff "$SYNC_RETRY_COUNT" "$SYNC_RETRY_DELAY" 300 \
        sync_to_dest \
        "s3://${bucket}${repo_path}/archive/${stanza}" \
        "/archive/${stanza}" \
        "${aws_sync_opts[@]}"

    info "Syncing backup for stanza: $stanza"
    retry_with_backoff "$SYNC_RETRY_COUNT" "$SYNC_RETRY_DELAY" 300 \
        sync_to_dest \
        "s3://${bucket}${repo_path}/backup/${stanza}/${last_backup_name}" \
        "/backup/${stanza}/${last_backup_name}" \
        "${aws_sync_opts[@]}"

    info "Syncing backup metadata"
    retry_with_backoff "$SYNC_RETRY_COUNT" "$SYNC_RETRY_DELAY" 60 \
        sync_to_dest \
        "s3://${bucket}${repo_path}/backup/${stanza}/backup.history" \
        "/backup/${stanza}/backup.history" \
        "${aws_sync_opts[@]}"

    retry_with_backoff "$SYNC_RETRY_COUNT" "$SYNC_RETRY_DELAY" 60 \
        copy_to_dest \
        "s3://${bucket}${repo_path}/backup/${stanza}/backup.info" \
        "/backup/${stanza}/backup.info" \
        "${aws_sync_opts[@]}"

    retry_with_backoff "$SYNC_RETRY_COUNT" "$SYNC_RETRY_DELAY" 60 \
        copy_to_dest \
        "s3://${bucket}${repo_path}/backup/${stanza}/backup.info.copy" \
        "/backup/${stanza}/backup.info.copy" \
        "${aws_sync_opts[@]}"
    
    # Post-sync validation
    if [ "$SYNC_SKIP_VALIDATION" = false ] && [ "$CBOB_DRY_RUN" != "true" ]; then
        info "Validating sync for stanza: $stanza"

        if is_dest_s3; then
            # For S3 destination, we verify the files exist in the destination
            if dest_exists "/backup/${stanza}/backup.info"; then
                info "✓ Backup metadata verified in S3 destination"
            else
                warning "Backup metadata not found in S3 destination for stanza: $stanza"
            fi
        else
            # For local destination, use pgBackRest validation
            if command -v pgbackrest &> /dev/null; then
                pgbackrest --stanza="$stanza" --log-level-console=warn info || {
                    warning "pgBackRest validation failed for stanza: $stanza"
                }
            else
                debug "pgBackRest not available, skipping validation"
            fi
        fi
    fi
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    # Record final storage metrics and calculate bytes synced
    record_storage_metrics "$cluster_id"
    bytes_after=$(get_metrics "storage" | jq -r ".[\"$cluster_id\"].total_size_bytes // 0" | tr -d '"')
    local bytes_synced=$((bytes_after - bytes_before))
    
    # Record sync metrics
    record_sync_metrics "$cluster_id" "$start_time" "$end_time" "success" "$bytes_synced"

    # Update tracking file
    update_sync_tracking "$cluster_id"

    info "✓ Sync completed for cluster $cluster_id (duration: ${duration}s, synced: $(human_readable_size $bytes_synced))"
}

# Main sync function
main() {
    parse_options "$@"

    # Handle --status flag early (before lock and validation)
    if [ "$SYNC_STATUS_ONLY" = true ]; then
        load_config
        echo "CBOB Sync Status"
        echo "================"
        echo ""

        # Fetch cluster info from API
        echo "Fetching clusters from Crunchy Bridge API..."
        local all_clusters_json=$(curl -s -f -X GET \
            "https://api.crunchybridge.com/clusters" \
            -H "Authorization: Bearer $CBOB_CRUNCHY_API_KEY" \
            -H "Accept: application/json" 2>/dev/null)

        if [ -n "$all_clusters_json" ]; then
            echo ""
            echo "All Clusters:"
            echo "$all_clusters_json" | jq -r '.clusters[] | "  \(.id) | \(.name) | \(if .parent_id then "REPLICA" else "PRIMARY" end)"'

            echo ""
            local total=$(echo "$all_clusters_json" | jq '.clusters | length')
            local primaries=$(echo "$all_clusters_json" | jq '[.clusters[] | select(.parent_id == null)] | length')
            local replicas=$(echo "$all_clusters_json" | jq '[.clusters[] | select(.parent_id != null)] | length')
            echo "Summary: $total total, $primaries primaries, $replicas replicas"

            if [ -n "${CBOB_IGNORE_CLUSTERS:-}" ]; then
                echo ""
                echo "Ignored clusters (CBOB_IGNORE_CLUSTERS):"
                IFS=',' read -ra ignored <<< "$CBOB_IGNORE_CLUSTERS"
                for id in "${ignored[@]}"; do
                    local name=$(echo "$all_clusters_json" | jq -r ".clusters[] | select(.id == \"$id\") | .name // \"unknown\"")
                    echo "  $id ($name)"
                done
            fi
        fi

        echo ""
        local tracking_file=$(get_sync_tracking_file)
        echo "Tracking file: $tracking_file"
        if [ -f "$tracking_file" ]; then
            echo ""
            echo "Last sync times (oldest first):"
            jq -r 'to_entries | sort_by(.value.last_sync) | .[] | "  \(.key): \(.value.last_sync_datetime // "never")"' "$tracking_file" 2>/dev/null
        else
            echo "No tracking file found. No syncs completed yet."
        fi
        exit 0
    fi

    # Acquire lock unless disabled
    if [ "${CBOB_NO_LOCK:-false}" != "true" ]; then
        acquire_lock "cbob_sync"
    fi

    # Validate environment
    validate_environment
    
    # Set up logging
    local log_file="${CBOB_LOG_PATH}/cbob_sync.log"
    mkdir -p "$(dirname "$log_file")"
    exec &> >(tee -a "$log_file")
    
    info "Crunchy Bridge Off-site Backup Sync v${CBOB_VERSION}"
    info "Starting at $(date)"

    # Determine which clusters to sync
    local clusters_to_sync=()
    if [ ${#SYNC_CLUSTERS[@]} -gt 0 ]; then
        # Use specified clusters from command line
        clusters_to_sync=("${SYNC_CLUSTERS[@]}")
        info "Using clusters specified via --cluster flag"
    elif [ -n "${CBOB_CRUNCHY_CLUSTERS:-}" ]; then
        # Legacy mode: use configured cluster list
        IFS=',' read -ra clusters_to_sync <<< "$CBOB_CRUNCHY_CLUSTERS"
        info "Using configured cluster list (legacy mode)"
    else
        # New mode: fetch all primary clusters, exclude ignored ones
        info "Auto-discovering clusters (ignoring replicas)..."
        local primary_clusters
        primary_clusters=$(get_primary_clusters "$CBOB_CRUNCHY_API_KEY" "${CBOB_IGNORE_CLUSTERS:-}")
        read -ra clusters_to_sync <<< "$primary_clusters"
    fi

    if [ ${#clusters_to_sync[@]} -eq 0 ]; then
        warning "No clusters to sync!"
        exit 0
    fi

    # Sort clusters by last sync time (oldest/never synced first)
    info "Sorting clusters by priority (oldest first)..."
    local sorted_clusters
    sorted_clusters=$(sort_clusters_by_priority "${clusters_to_sync[@]}")
    read -ra clusters_to_sync <<< "$sorted_clusters"

    # Show current sync status
    show_sync_status

    info "Clusters to sync (priority order): ${clusters_to_sync[*]}"

    # Build notification context
    local server_name=$(hostname -f 2>/dev/null || hostname)
    local dest_info
    if is_dest_s3; then
        dest_info="${CBOB_DEST_ENDPOINT} → ${CBOB_DEST_BUCKET}"
    else
        dest_info="local: ${CBOB_TARGET_PATH}"
    fi

    notify ":arrows_counterclockwise: CBOB Sync starting
• Server: ${server_name}
• Destination: ${dest_info}
• Clusters: ${#clusters_to_sync[@]}"
    
    # Sync clusters
    local failed_clusters=()
    local success_count=0
    
    if [ "$SYNC_PARALLEL" -gt 1 ]; then
        info "Running parallel sync with $SYNC_PARALLEL workers"
        
        # Use GNU parallel if available, otherwise fall back to sequential
        if command -v parallel &> /dev/null; then
            export -f sync_cluster
            export -f info warning error debug
            export -f retry_with_backoff
            export -f human_readable_size
            
            printf '%s\n' "${clusters_to_sync[@]}" | \
                parallel -j "$SYNC_PARALLEL" sync_cluster {} || true
        else
            warning "GNU parallel not found, falling back to sequential sync"
            SYNC_PARALLEL=1
        fi
    fi
    
    if [ "$SYNC_PARALLEL" -eq 1 ]; then
        # Sequential sync
        for cluster_id in "${clusters_to_sync[@]}"; do
            if sync_cluster "$cluster_id"; then
                ((++success_count))
            else
                failed_clusters+=("$cluster_id")
            fi
        done
    fi
    
    # Send heartbeat if configured
    if [ -n "${CBOB_SYNC_HEARTBEAT_URL:-}" ]; then
        send_heartbeat "$CBOB_SYNC_HEARTBEAT_URL" "sync_complete"
    fi
    
    # Summary
    info "Sync summary:"
    info "  Total clusters: ${#clusters_to_sync[@]}"
    info "  Successful: $success_count"
    info "  Failed: ${#failed_clusters[@]}"
    
    if [ ${#failed_clusters[@]} -gt 0 ]; then
        warning "Failed clusters: ${failed_clusters[*]}"
        exit 1
    fi
    
    info "✓ All backups synced successfully!"
    notify ":white_check_mark: CBOB Sync completed!
• Server: ${server_name}
• Destination: ${dest_info}
• Clusters: ${#clusters_to_sync[@]} synced successfully"
}

# Run main function
main "$@"